<head>
<link href="https://fonts.googleapis.com/css?family=Montserrat" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Libre+Baskerville" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Muli" rel="stylesheet">

<link rel="stylesheet" href="css/general.css">
<link rel="stylesheet" href="css/general_visualizations.css">

<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>

<script src="js/example_tweets.js" type="text/javascript"></script>
<script src="js/scores_per_tweet.js" type="text/javascript"></script>
<script src="js/predictions_per_language_model.js" type="text/javascript"></script>

</head>

<body>
<div class="header">
	<h1>How language prediction works</h1>
</div>

<div id="main">

<img class="avatar" src="avatar.PNG">
<p class="name">Wessel Stoop</p>

<p class="lead">If you have ever typed something on a smartphone, you have used text prediction. In this article, we will see how text predictors can work and how finding the correct example language dataset is crucial for what kind of predictions come out. To see how this all works, we will try to predict tweets by four famous Twitter users.</p>

<p><span id="first_character">T</span>o be able to make useful predictions, the text predictor needs as much knowledge about language as possible. Adding this knowledge by hand would be an enormous task, so this is often done by <em>machine learning</em>. These days, machine learning algorithms called <em>neural networks</em> are the best at text prediction, because of their remarkable ability to <a>capture meaning in a list of numbers</a> and to learn about the <a>hierarchical levels of language</a> (from letters, to syllables, to words, phrases, sentences, etc). The downside, however, is that they can be pretty difficult to grasp - understanding why neural networks do the things they do is a <a>whole research field on its own</a>. </p> 

<p>Fortunately, there are surprisingly effective machine learning techniques that are much simpler. For example, the algorithm <em>K Nearest Neighbours</em> works by looking at the last few words you wrote, comparing these to all groups of words seen during the training phase, and giving as the best guess what followed groups of similar words in the past. Let's see how that works for the tweets of 4 <a href="https://en.wikipedia.org/wiki/List_of_most-followed_Twitter_accounts#Top_50_accounts">of the most followed Twitter users on Earth</a>. You can interact and play around with the results in the panel below:</p>

<div></div>

<p>Often, there is simply no similar group of words available to make a prediction. In that case, we just try to make a prediction based on a list of words frequently used by the Twitter user; these situations are blue in the panel above. The result is what you could call a very simple statistical language model. It's built entirely of example language data: if you put in different data, you get another model with other predictions. You can see that clearly if you let the models of the various celebrities predict each other's tweets:</p>

<div class="visualization" id="models_vis">
	<script src="js/models_visualization.js" type="text/javascript"></script>

	<div class="visualization_section">
		<div class="section_title">
			Twitter user
		</div>
		<img class="user_select profile_picture selected" user="barackobama" src="img/barackobama.jpg">
		<img class="user_select profile_picture" user="jtimberlake" src="img/jtimberlake.jpg">
		<img class="user_select profile_picture" user="kimkardashian" src="img/kimkardashian.jpg">
		<img class="user_select profile_picture" user="ladygaga" src="img/ladygaga.jpg">

	</div>

	<div class="visualization_section">
		<div class="section_title">
			Example tweet
		</div>

		<a class="tweet_select selected" tweet_index=1>1</a>
		<a class="tweet_select" tweet_index=2>2</a>
		<a class="tweet_select" tweet_index=3>3</a>
		<a class="tweet_select" tweet_index=4>4</a>
		<a class="tweet_select" tweet_index=5>5</a>
		<div class="embedded_tweet_area"></div>

	</div>

	<div class="visualization_section">
		<div class="section_title">
			Slide back and forth
		</div>
		<div class="slider_area">
			<div class="arrow_left"></div><input type="range" min="1" max="10" value="2" class="slider" id="progress_slider"></input><div class="arrow_right"></div>
		</div>

		<p class="predicted_text">This is|</p>
	</div>

	<div class="visualization_section">
		<div class="section_title">
			Best predictions by language models
		</div>

		<table>
			<tr>
				<td><img class="predictor_picture profile_picture" src="img/barackobama.jpg"></td><td><div class="predictor_name">@barackobama</div><div id="barackobama_predictions"></div></td>
			</tr>
			<tr>
				<td><img class="predictor_picture profile_picture" src="img/jtimberlake.jpg"></td><td><div class="predictor_name">@jtimberlake</div><div id="jtimberlake_predictions"></div></td>
			</tr>
			<tr>
				<td><img class="predictor_picture profile_picture" src="img/kimkardashian.jpg"></td><td><div class="predictor_name">@kimkardashian</div><div id="kimkardashian_predictions"></div></td>
			</tr>
			<tr>
				<td><img class="predictor_picture profile_picture" src="img/ladygaga.jpg"></td><td><div class="predictor_name">@ladygaga</div><div id="ladygaga_predictions"></div></td>
			</tr>
		</table>
	</div>
</div>

<p>You might have noticed that the model of the user itself is the one that makes the correct prediction most often. This follows from the more general phenomenon in machine learning that the more similar the training data is to the production data, the better the results will be. Or, more simply: nobody writes more like you than you, so you are your own best language predictor.</p>

<p>Instead of just eyeballing which model works better, we can actually measure it and count the number of correctly guessed characters. You could argue that we are too harsh on ourselves if we count this way, as most language prediction apps give multiple options, but it's at least a good tool to see which language models work best.</p>

<div></div>

<p>Twitter is the perfect resource to show the power of personalized models: it's effectively a huge collection of language organized per author. However, our example users so far have been really active Twitter users with often thousands of Tweets. For most people, we don't have that much material. How do we solve that? We have learned above that we need example language similar to the language we are trying to predict... but where to find that? Sociolinguistics has the answer: people who talk to each other a lot start to speak alike. Fun fact: 'people who talk to each other a lot' can be approximated on Twitter by following the @ mentions; here's a list of the other Twitter users our Twitter users appear to be talking to most:</p>

<p>Big question now is: how do these 'friend' (or in the case of Khlo&eacute;   Kardashian: 'sister') models perform? Judge for yourself:</p>

<div></div>

</body>